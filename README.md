<p align="center">
  <img width="307" alt="image" src="https://user-images.githubusercontent.com/56249874/132158755-d02dcd81-2edf-48c0-9f48-7475a231306f.png">
</p>
<p align="center">
  <a href="https://github.com/nishiwen1214/DropAttack/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/misitebao/standard-repository?style=flat-square"/></a>
  <a href="https://github.com/nishiwen1214/DropAttack"><img alt="GitHub Repo stars"src="https://img.shields.io/github/stars/nishiwen1214/DropAttack?style=flat-square"/></a>
  <a href="https://github.com/nishiwen1214/"><img alt="GitHub Repo stars" src="https://img.shields.io/badge/author-nishiwen1214-brightgreen?style=flat-square"/></a>
</p>

<span id="nav-1"></span>
## DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks
Abstract: Adversarial training has been proven to be a powerful regularization method to improve the generalization of models. However, current adversarial training methods only attack the original input sample or the embedding vectors, and their attacks lack coverage and diversity. To further enhance the breadth and depth of attack, we propose a novel masked weight adversarial training method called DropAttack, which enhances generalization of model by adding intentionally worst-case adversarial perturbations to both the input and hidden layers in different dimensions and minimize the adversarial risks generated by each layer. DropAttack is a general technique and can be adopt to a wide variety of neural networks with different architectures. To validate the effectiveness of the proposed method, we used five public datasets in the fields of natural language processing (NLP) and computer vision (CV) for experimental evaluating. We compare the proposed method with other adversarial training methods and regularization methods, and our method achieves state-of-the-art on all datasets. In addition, Dropattack can achieve the same performance when it use only a half training data compared to other standard training method. Theoretical analysis reveals that DropAttack can perform gradient regularization at random on some of the input and wight parameters of the model. Further visualization experiments show that DropAttack can push the minimum risk of the model to a lower and flatter loss landscapes.

- #### For technical details and additional experimental results, please refer to our paper:

[DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks](http://arxiv.org/abs/2108.12805)

 <img width="770" alt="image" src="https://user-images.githubusercontent.com/56249874/131253058-b2efc608-f3e8-4633-977c-77e1c3f1e426.png">
 
- #### Experimental results:
<img width="916" alt="image" src="https://user-images.githubusercontent.com/56249874/131650037-5bdf978d-5ac1-4da2-9cac-aa7bda538b1b.png">

<img width="943" alt="image" src="https://user-images.githubusercontent.com/56249874/132156985-d807e42a-23cf-4a1b-a457-396dc363c0e4.png">

DropAttack indeed selects flatter loss landscapes via masked adversarial perturbations.
<img width="981" alt="image" src="https://user-images.githubusercontent.com/56249874/132156881-5d7f059f-802b-4391-94b2-8bfd26b7e9b6.png">

- ### Requirements
```
pytorch
pandas
numpy
nltk
sklearn
torchtext
```

- ### Please star it, thank you! :ï¼‰
